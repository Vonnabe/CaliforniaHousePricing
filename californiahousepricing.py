# -*- coding: utf-8 -*-
"""CaliforniaHousePricing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rStMBGNNHI4M0Ora81WBX4WqHzcoTMSU
"""

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!kaggle datasets download -d camnugent/california-housing-prices

!unzip california-housing-prices.zip

import pandas as pd

housing_pd = pd.read_csv('housing.csv')
housing_pd.head()

housing_pd['ocean_proximity'].value_counts()

shuffled_pd = housing_pd.sample(n=len(housing_pd), random_state=1)
shuffled_pd

pd.get_dummies(shuffled_pd['ocean_proximity']).head()

shuffled_pd.drop('ocean_proximity', axis=1).head()

final_pd = pd.concat([shuffled_pd.drop('ocean_proximity', axis=1),
                      pd.get_dummies(shuffled_pd['ocean_proximity'])], axis=1)
final_pd

final_pd = final_pd[['longitude',	'latitude',
                     'housing_median_age',	'total_rooms',
                     'total_bedrooms', 'population',
                     'households',	'median_income', '<1H OCEAN',
                     'INLAND', 'ISLAND', 'NEAR BAY',
                     'NEAR OCEAN', 'median_house_value']]
final_pd

final_pd = final_pd.dropna()
len(final_pd)

train_pd, test_pd, val_pd = final_pd[:18000], final_pd[18000:19215], final_pd[19215:]
len(train_pd), len(test_pd), len(val_pd)

x_train, y_train = train_pd.to_numpy()[:, :-1], train_pd.to_numpy()[:, -1]
x_val, y_val = val_pd.to_numpy()[:, :-1], val_pd.to_numpy()[:, -1]
x_test, y_test = test_pd.to_numpy()[:, :-1], test_pd.to_numpy()[:, -1]

x_train.shape, y_train.shape, x_val.shape, y_val.shape, x_test.shape, y_test.shape

from sklearn.preprocessing import StandardScaler
import numpy as np

scaler= StandardScaler().fit(x_train[:, :8])

def preprocessor(X):
  A = np.copy(X)
  A[:, :8] = scaler.transform(A[:, :8])
  return A

x_train, x_val, x_test = preprocessor(x_train), preprocessor(x_val), preprocessor(x_test)
x_train.shape, x_val.shape, x_test.shape

pd.DataFrame(x_train).head()

pd.DataFrame(x_train).hist()

from sklearn.metrics import mean_squared_error as mse
from sklearn.linear_model import LinearRegression

lm = LinearRegression().fit(x_train, y_train)
mse(lm.predict(x_train), y_train, squared=False), mse(lm.predict(x_val), y_val, squared=False)

from sklearn.neighbors import KNeighborsRegressor

knn = KNeighborsRegressor(n_neighbors=10).fit(x_train, y_train)
mse(knn.predict(x_train), y_train, squared=False), mse(knn.predict(x_val), y_val, squared=False)

from sklearn.ensemble import RandomForestRegressor

rfr = RandomForestRegressor(max_depth=10).fit(x_train, y_train)
mse(rfr.predict(x_train), y_train, squared=False), mse(rfr.predict(x_val), y_val, squared=False)

from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(n_estimators=250).fit(x_train, y_train)
mse(gbr.predict(x_train), y_train, squared=False), mse(gbr.predict(x_val), y_val, squared=False)

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

simple_nn = Sequential()
simple_nn.add(InputLayer((13,)))
simple_nn.add(Dense(2, 'relu'))
simple_nn.add(Dense(1, 'linear'))

opt = Adam(learning_rate=.1)
cp = ModelCheckpoint('models/simple_nn', save_best_only=True)
simple_nn.compile(optimizer=opt, loss='mse', metrics=[RootMeanSquaredError()])
simple_nn.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), callbacks=[cp], epochs=100)

from tensorflow.keras.models import load_model

simple_nn = load_model('models/simple_nn')
mse(simple_nn.predict(x_train), y_train, squared=False), mse(simple_nn.predict(x_val), y_val, squared=False)

medium_nn = Sequential()
medium_nn.add(InputLayer((13,)))
medium_nn.add(Dense(32, 'relu'))
medium_nn.add(Dense(16, 'relu'))
medium_nn.add(Dense(1, 'linear'))

opt = Adam(learning_rate=.1)
cp = ModelCheckpoint('models/medium_nn', save_best_only=True)
medium_nn.compile(optimizer=opt, loss='mse', metrics=[RootMeanSquaredError()])
medium_nn.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), callbacks=[cp], epochs=100)

medium_nn = load_model('models/simple_nn')
mse(medium_nn.predict(x_train), y_train, squared=False), mse(medium_nn.predict(x_val), y_val, squared=False)

large_nn = Sequential()
large_nn.add(InputLayer((13,)))
large_nn.add(Dense(256, 'relu'))
large_nn.add(Dense(128, 'relu'))
large_nn.add(Dense(64, 'relu'))
large_nn.add(Dense(32, 'relu'))
large_nn.add(Dense(1, 'linear'))

opt = Adam(learning_rate=.1)
cp = ModelCheckpoint('models/large_nn', save_best_only=True)
large_nn.compile(optimizer=opt, loss='mse', metrics=[RootMeanSquaredError()])
large_nn.fit(x=x_train, y=y_train, validation_data=(x_val, y_val), callbacks=[cp], epochs=100)

large_nn = load_model('models/simple_nn')
mse(large_nn.predict(x_train), y_train, squared=False), mse(large_nn.predict(x_val), y_val, squared=False)

mse(gbr.predict(x_test), y_test, squared=False)